{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# EN.520.637 Foundations of Reinforcement Learning\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"darkblue\">Lab 3: Dynamic Programming and Monte Carlo Method  </font></h1>\n",
    "\n",
    "\n",
    "\n",
    "## Deadline\n",
    "11:59 pm Oct 26, 2020 (12 days)\n",
    "\n",
    "##  Content\n",
    "1. Gym environment\n",
    "2. Dynamic Programming (GridWorld)\n",
    "3. Monte Carlo Method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gym library\n",
    "\n",
    "\n",
    "Gym is a toolkit for developing and comparing reinforcement learning algorithms, (see https://gym.openai.com/). A bunch of [classic RL problems](https://gym.openai.com/envs/#classic_control) could be easily simulated. In particular, Gym environment provid all necessary variables (e.g. current state, next state and step reward) and the only thing remains for you to do is choosing action based on different algorithms, (please read https://gym.openai.com/docs/ for more details).\n",
    "\n",
    "Run the following code to install:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from IPython import display\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dynamic Programming (GridWorld)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Intro to GridWorld\n",
    "\n",
    "In this section, we apply policy iteration and value iteration to solve to gridworld problem. \n",
    "\n",
    "The grid is shown below, the black tiles represents wall/obstacles, the white tiles are the non-terminal tiles, and the tile with \"s\" is the starting point of every episoid, the tile with \"5\" is the goal point.\n",
    "\n",
    "The agent start at \"s\" tile. At every step, the agent can choose one of the four actions:\"up\",\"right\",\"down\",\"left\", moving to the next tile in that direction. \n",
    "\n",
    "$\\cdot$ If the next tile is wall/obstacle, the agent does not move and receive -1 reward;\n",
    "\n",
    "$\\cdot$ If the next tile is a non-terminal tile, the agent move to that tile and receive 0 reward;\n",
    "\n",
    "$\\cdot$ If the next tile is the goal tile, the episoid is finished and the agent receive 5 reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gridworld import GridWorld\n",
    "\n",
    "gw = GridWorld()\n",
    "gw.plot_grid(plot_title='The grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 States and state values\n",
    "Excluding the wall around the grid, there are 56 tiles (INCLUDING obstacles inside the grid), and they correspond to 56 states (obstacles and goal are non-reachable states).\n",
    "\n",
    "We use numbers from 0 to 55 to represent these states (see gridworld.py for the coversion between integer and tile position). The correspondance are as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gw.plot_state_values(np.arange(56),value_format=\"{:d}\",plot_title='Each states as an integer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Take actions\n",
    "Use GridWorld.step(action) to take an action, and use GridWorld.reset() to restart an episoid\n",
    "\n",
    "action is an integer from 0 to 3\n",
    "\n",
    "0: \"Up\"; 1: \"Right\"; 2: \"Down\"; 3: \"Left\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current state is 9, which corresponds to tile position (1, 1)\n",
      "\n",
      "Take action 2, get reward 0.0, move to state 17\n",
      "Now the current state is 17, which corresponds to tile position (2, 1)\n",
      "\n",
      "Reset episode\n",
      "Now the current state is 9, which corresponds to tile position (1, 1)\n"
     ]
    }
   ],
   "source": [
    "gw.reset()\n",
    "\n",
    "current_state = gw.get_current_state()\n",
    "tile_pos = gw.int_to_state(current_state)\n",
    "\n",
    "print(\"The current state is {}, which corresponds to tile position {}\\n\".format(current_state,tile_pos))\n",
    "\n",
    "action = np.random.randint(4)\n",
    "reward, terminated, next_state = gw.step(action)\n",
    "tile_pos = gw.int_to_state(next_state)\n",
    "\n",
    "print(\"Take action {}, get reward {}, move to state {}\".format(action,reward,next_state))\n",
    "print(\"Now the current state is {}, which corresponds to tile position {}\\n\".format(next_state,tile_pos))\n",
    "\n",
    "gw.reset()\n",
    "current_state = gw.get_current_state()\n",
    "tile_pos = gw.int_to_state(current_state)\n",
    "print(\"Reset episode\")\n",
    "print(\"Now the current state is {}, which corresponds to tile position {}\".format(current_state,tile_pos))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Plot Deterministic Policies\n",
    "A deterministic policy is a function from state to action, which can be represented by a (56,)-numpy array whose entries are all integers in (0-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gw.plot_policy(np.random.randint(4,size=(56,)),plot_title='A deterministic policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Policy Evaluation\n",
    "\n",
    "Recall that the value function $v_\\pi(s)$ of a policy $\\pi(s)$ can be iteratively computed via Policy Evaluation (See Sutton&Barto Section 4.1), the iteration is given by $$v_{k+1}(s)=\\sum_{a}\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_{k}(s)]\\,,$$ which can be written as $$\n",
    "    v_{k+1}(s)=\\sum_{a}\\pi(a|s)\\left[\\mathbb{E}_\\pi[r|s,a]+\\sum_{s'}p(s'|s,a)v_k(s')\\right]\\,.\n",
    "$$\n",
    "If we write value function $v_{k+1},v_k$ as vectors, then we have\n",
    "$$\n",
    "    v_{k+1} = \\sum_{a}\\pi(a|s)\\left[R_\\pi(a)+P_\\pi(a)v_{k}\\right]\\,.\n",
    "$$\n",
    "where $R_\\pi(a)$ is the expected reward under action $a$ and  $P_\\pi(a)$ is the transition probability matrix under action $a$. \n",
    "\n",
    "With this we can find the value function $v_\\pi$ of random policy $\\pi$ when discount is 0.9, the code is shown below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_a = [0.25,0.25,0.25,0.25] # action proability of random action (for any state)\n",
    "\n",
    "gamma = 0.9\n",
    "max_it = 1000\n",
    "tol = 1e-5\n",
    "v = np.zeros((56,))\n",
    "for i in range(max_it):\n",
    "    value_temp = np.zeros((56,))\n",
    "    for action in range(4):\n",
    "        reward, tran_prob = gw.transition(action)\n",
    "        value_temp = value_temp + pi_a[action]*(reward+gamma* np.matmul(tran_prob,v))\n",
    "    if np.linalg.norm(value_temp-v)<tol:\n",
    "        break\n",
    "    else:\n",
    "        v = value_temp\n",
    "        \n",
    "v_final = v\n",
    "\n",
    "gw.plot_state_values(v,value_format=\"{:.1f}\",plot_title='Value function of Random Policy')\n",
    "\n",
    "# compute and plot the greedy policy of this value function (Hint: first compute Q from v)\n",
    "\n",
    "# write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Value Iteration\n",
    "\n",
    "Implement Value Iteration Algorithm (Sutton&Barto Section 4.4) to find the optimal policy of this gridworld, and plot its value function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration Algorithm\n",
    "\n",
    "def value_iteration(gridworld, gamma):\n",
    "# input:  gridworld, (GridWorld class) gridworld class describing the environment\n",
    "#         gamma,     (float 0-1) discount of the return\n",
    "# output: optim value,  (1d numpy array, float) optimal value function \n",
    "#         optim_policy, (1d numpy array, int {0,1,2,3}) optimal policy\n",
    "\n",
    "#write your code here\n",
    "    \n",
    "\n",
    "    return optim_value, optim_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your result here\n",
    "optim_value, optim_policy = value_iteration(gw, 0.9)\n",
    "gw.plot_state_values(optim_value,value_format=\"{:.1f}\",plot_title='Value function of Random Policy')\n",
    "gw.plot_policy(optim_policy,plot_title='Greedy Policy of Random Policy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monte Carlo Method  (CartPole-v1 environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 CartPole Introduction\n",
    "\n",
    "We now apply Monte Carlo Method to the CartPole problem. \n",
    "\n",
    "\n",
    "1. A pole is attached via an un-actuated joint to a cart, which moves along a frictionless track. \n",
    "\n",
    "0. The system is controlled by applying a force of +1 or -1 to the cart. \n",
    "\n",
    "0. The pole starts at upright position, and the goal is to prevent it from falling over. \n",
    "\n",
    "0. A reward of +1 is obtained for every timestep that the pole remains upright. \n",
    "\n",
    "0. The episode ends when the pole is more than 15 degrees from the vertical, or the cart moves more than 2.4 units from the center.\n",
    "\n",
    "0. For more info (See [SOURCE ON GITHUB](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py)).\n",
    "\n",
    "The following examples show the basic usage of this testing environment: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Episode initialization and Initial Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inital observation is [ 0.03472954 -0.02706774  0.01980837 -0.04862776]\n",
      "\n",
      "This means the cart current position is 0.03472954227217041 with velocity -0.027067735180598497,\n",
      "and the pole current angular position is 0.01980837406919167 with angular velocity -0.048627761309786924,\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset() ##Initial an episode\n",
    "\n",
    "print(\"Inital observation is {}\".format(observation))\n",
    "\n",
    "print(\"\\nThis means the cart current position is {}\".format(observation[0]), end = '')\n",
    "print(\" with velocity {},\".format(observation[1]))\n",
    "\n",
    "print(\"and the pole current angular position is {}\".format(observation[2]), end = '')\n",
    "print(\" with angular velocity {},\".format(observation[3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Take actions\n",
    "\n",
    "\n",
    "Use env.step(action) to take an action\n",
    "\n",
    "action is an integer from 0 to 1\n",
    "\n",
    "0: \"Left\"; 1: \"Right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current observation is [ 0.03472954 -0.02706774  0.01980837 -0.04862776]\n",
      "\n",
      "New observation is [ 0.03418819 -0.22246804  0.01883582  0.25023847]\n",
      "Step reward is 1.0\n",
      "Did episode just ends? -False\n"
     ]
    }
   ],
   "source": [
    "print(\"Current observation is {}\".format(observation))\n",
    "\n",
    "action = 0 #go left\n",
    "observation, reward, done, info = env.step(action) # simulate one step\n",
    "\n",
    "print(\"\\nNew observation is {}\".format(observation))\n",
    "print(\"Step reward is {}\".format(reward))\n",
    "print(\"Did episode just ends? -{}\".format(done)) # episode ends when 3.1(6) happens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Simulate multiple episodes\n",
    "\n",
    "(You may uncomment those lines to see an animation. However, it will not work for JupyterHub since the animation requires GL instead of webGL. If you have Jupyter notebook localy on your computer, this version of code will work through a virtual frame.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward per episode is 20.833333333333332\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "total_reward = 0\n",
    "ep_num = 0\n",
    "# img = plt.imshow(env.render(mode='rgb_array')) \n",
    "\n",
    "\n",
    "for _ in range(1000):\n",
    "    #     img.set_data(env.render(mode='rgb_array')) \n",
    "    #     display.display(plt.gcf())\n",
    "    #     display.clear_output(wait=True)\n",
    "    \n",
    "    action = env.action_space.sample()     # this takes random actions\n",
    "    observation, reward, done, info = env.step(action) \n",
    "       \n",
    "    total_reward += reward\n",
    "    \n",
    "\n",
    "\n",
    "    if done:                               # episode just ends\n",
    "        observation = env.reset()          # reset episode\n",
    "        ep_num += 1\n",
    "\n",
    "print(\"Average reward per episode is {}\".format(total_reward/ep_num))\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.4 States Discretization \n",
    "\n",
    "The class DiscreteObs() discretizes the observation space into discrete state space, based on numpy.digitize (Please read its description in https://numpy.org/doc/stable/reference/generated/numpy.digitize.html) \n",
    "\n",
    "Discretization of observation space is necessary for tabular methods. You can use DiscreteObs() or any other library for discretizing the observation space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current position of the cart is -0.0460\n",
      "\n",
      "Current velocity of the cart is -0.0451\n",
      "\n",
      "Current angular position of the pole is -0.0479 rad\n",
      "\n",
      "Current angular velocity of the pole is -0.0100 rad\n",
      "\n",
      "which are mapped to state [20, 2, 16, 2], with corresponding index 24210\n"
     ]
    }
   ],
   "source": [
    "class DiscretObs():\n",
    "    \n",
    "    \n",
    "    def __init__(self, bins_list):\n",
    "        self._bins_list = bins_list\n",
    "        \n",
    "        self._bins_num = len(bins_list)\n",
    "        self._state_num_list = [len(bins)+1 for bins in bins_list]\n",
    "        self._state_num_total = np.prod(self._state_num_list)\n",
    "    \n",
    "    def get_state_num_total(self):\n",
    "        \n",
    "        return self._state_num_total\n",
    "    \n",
    "    def obs2state(self, obs):\n",
    "        \n",
    "        if not len(obs)==self._bins_num:\n",
    "            raise ValueError(\"observation must have length {}\".format(self._bins_num))\n",
    "        else:\n",
    "            return [np.digitize(obs[i], bins=self._bins_list[i]) for i in range(self._bins_num)]\n",
    "        \n",
    "    def obs2idx(self, obs):\n",
    "        \n",
    "        state = self.obs2state(obs)\n",
    "        \n",
    "        return self.state2idx(state)\n",
    "    \n",
    "    def state2idx(self, state):\n",
    "        \n",
    "        idx = 0\n",
    "        for i in range(self._bins_num-1,-1,-1):\n",
    "            idx = idx*self._state_num_list[i]+state[i]\n",
    "        \n",
    "        return idx\n",
    "    \n",
    "    def idx2state(self, idx):\n",
    "        \n",
    "        state = [None]*self._bins_num\n",
    "        state_num_cumul = np.cumprod(self._state_num_list)\n",
    "        for i in range(self._bins_num-1,0,-1):\n",
    "            state[i] = idx/state_num_cumul[i-1]\n",
    "            idx -=state[i]*state_num_cumul[i-1]\n",
    "        state[0] = idx%state_num_cumul[0]\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Recommended Discretization for Carpole-v1 when using Monte-Carlo methods\n",
    "bins_pos = np.linspace(-2.4,2.4,40)        # position\n",
    "bins_d_pos = np.linspace(-3,3,5)           # velocity\n",
    "bins_ang = np.linspace(-0.2618,0.2618,40)  # angle\n",
    "bins_d_ang = np.linspace(-0.3,0.3,5)       # angular velocity\n",
    "\n",
    "dobs = DiscretObs([bins_pos,bins_d_pos,bins_ang,bins_d_ang])\n",
    "observation = env.reset()\n",
    "\n",
    "state = dobs.obs2state(observation)\n",
    "idx = dobs.obs2idx(observation)\n",
    "\n",
    "print(\"Current position of the cart is {:.4f}\\n\".format(observation[0]))\n",
    "print(\"Current velocity of the cart is {:.4f}\\n\".format(observation[1]))\n",
    "print(\"Current angular position of the pole is {:.4f} rad\\n\".format(observation[2]))\n",
    "print(\"Current angular velocity of the pole is {:.4f} rad\\n\".format(observation[3]))\n",
    "\n",
    "print(\"which are mapped to state {}, with corresponding index {}\".format(state,idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 On-policy first-visit MC control\n",
    "\n",
    "1. Implement \"On-policy first-visit MC control\" algorithum in [Chap. 5.4 Sutton&Barto]\n",
    "2. Simulate this algorithm for 40000 episodes.\n",
    "3. Divide the previous 40000 episodes into 20 sets. Plot the average reward for each set. (i.e. plot the average reward for the first 2000 episodes, the second 2000 episodes, ..., and the 15th 2000 episodes.) \n",
    "4. Use greedy policy of the trained Q function to control the carpole for 100 episodes, plot the accumulate reward over 100 episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested functions (Feel free to modify existing and add new functions)\n",
    "\n",
    "def get_action(current_state, Q, epsilon):\n",
    "    \n",
    "    # Choose optimal action based on current state and Q\n",
    "    #\n",
    "    # input:  current state,  (array) \n",
    "    #         Q,              (array)  \n",
    "    #         epsilonn,       (float)  \n",
    "    # output: action\n",
    "    #         \n",
    "    return action\n",
    "\n",
    "\n",
    "\n",
    "def update_Q(Q, observation_list, action_list):\n",
    "    # Update Q at the end of each episode\n",
    "    #\n",
    "    # input:  current Q, (array) \n",
    "    #         observation_list,       (array)  states observed in this episode\n",
    "    #         action_list,       (array)  actions took in this spisode\n",
    "    # output: Updated Q\n",
    "    #         \n",
    "\n",
    "        \n",
    "    return Q\n",
    "\n",
    "\n",
    "## Suggested flow (Feel free to modify and add)\n",
    "\n",
    "# parameters for epsilon-greedy algorithm, when epsilon_decay_rate=1, the algorothm implement a fixed \n",
    "# epsilon value as epsilon_start, you can choose either fixed epsilon or decaying epsilon\n",
    "\n",
    "# epsilon_start = 0.3\n",
    "# epsilon_decay_rate = 0.97\n",
    "\n",
    "set_num = 30\n",
    "s = 0\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "\n",
    "#epsilon = epsilon_start   # set epsilon\n",
    "\n",
    "while 1:\n",
    "    \n",
    "    \n",
    "    current_state =                             # discretize the observation space\n",
    "    \n",
    "    action = get_action(current_state,Q,epsilon)# pick action by epsilon greedy policy\n",
    "    \n",
    "    observation, reward, done, info = env.step(action) # simulate one step\n",
    "    \n",
    "    if done:  # end of epsode\n",
    "        Q = update_Q(Q, observation_list, action_list) # update Q for past observations in the episode\n",
    "        \n",
    "        ep_num += 1\n",
    "        \n",
    "        if  np.mod(ep_num,2000)==0: # end of every set of episode\n",
    "            \n",
    "            #epsilon = epsilon*epsilon_decay_rate     # update epsilon\n",
    "            s+=1\n",
    "            \n",
    "            if s == set_num:\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your result here (Feel free to modify)\n",
    "# the result_mc should be a (set_num, )-numpy array that records the average reward of a set of episodes \n",
    "\n",
    "# put your result here\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 15}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "figure(figsize=(12,6))\n",
    "ax = subplot(1,1,1)\n",
    "ax.plot(range(0,set_num), result_mc, linewidth=2, color='g')\n",
    "\n",
    "plt.ylabel(\"Set average reward\");\n",
    "plt.xlabel(\"Set number (2000 episodes simulated in each set)\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use greedy policy of the trained Q function to control the carpole for 100 episode, \n",
    "# and plot the total reward received in each episode\n",
    "## Suggested flow (Feel free to modify and add)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "\n",
    "# img = plt.imshow(env.render(mode='rgb_array')) \n",
    "\n",
    "count = 0\n",
    "while 1:\n",
    "    #     img.set_data(env.render(mode='rgb_array')) \n",
    "    #     display.display(plt.gcf())\n",
    "    #     display.clear_output(wait=True)\n",
    "    \n",
    "    current_state =                             # discretize the observation space\n",
    "    \n",
    "    action =                                    # choose action by greedy policy of the trained Q\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    \n",
    "    if done:\n",
    "        \n",
    "        observation = env.reset()\n",
    "        count +=1\n",
    "        \n",
    "        total_reward_mc =                                # record the total reward until this episode\n",
    "        \n",
    "        if count==100:\n",
    "            break\n",
    "        \n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 15}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "figure(figsize=(12,6))\n",
    "ax = subplot(1,1,1)\n",
    "ax.plot(range(100), total_reward_mc)\n",
    "\n",
    "plt.ylabel(\"Accumulate Reward\");\n",
    "plt.xlabel(\"Episode\");\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 MC Control with partially observed states\n",
    "\n",
    "Now suppose we are only able to observe the position of the cart and angle of the pole, i.e. observation[0] and observation[2]. Using same state discretization for these two states in 3.2, run MC control:\n",
    "\n",
    "1.Simulate this algorithm for 40000 episodes.\n",
    "\n",
    "2.Divide the previous 40000 episodes into 20 sets. Plot average rewards for each sets. (i.e. plot average rewards for the first 2000 episodes, the second 2000 episodes, ..., and the 15th 2000 episodes.).\n",
    "\n",
    "3.Use greedy policy of the trained Q function to control the carpole for 100 episode, plot the accumulate rewards over 100 episodes, and compare it with one from 3.2 in the same figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Suggested flow (Feel free to modify and add)\n",
    "\n",
    "# parameters for epsilon-greedy algorithm, when epsilon_decay_rate=1, the algorothm implement a fixed \n",
    "# epsilon value as epsilon_start, you can choose either fixed epsilon or decaying epsilon\n",
    "\n",
    "# epsilon_start = 0.3\n",
    "# epsilon_decay_rate = 0.97\n",
    "\n",
    "set_num = 30\n",
    "s = 0\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "observation = env.reset()\n",
    "observation = observation([[0,2]])  # partially observed states\n",
    "\n",
    "#epsilon = epsilon_start   # set epsilon\n",
    "\n",
    "while 1:\n",
    "    \n",
    "    \n",
    "    current_state =                             # discretize the observation space\n",
    "    \n",
    "    action = get_action(current_state,Q,epsilon)# pick action by epsilon greedy policy\n",
    "    \n",
    "    observation, reward, done, info = env.step(action) # simulate one step\n",
    "    observation = observation([[0,2]])  # partially observed states\n",
    "    \n",
    "    if done:  # end of epsode\n",
    "        Q = update_Q(Q, observation_list, action_list) # update Q for past observations in the episode\n",
    "        \n",
    "        ep_num += 1\n",
    "        \n",
    "        if  np.mod(ep_num,2000)==0: # end of every set of episode\n",
    "            \n",
    "            #epsilon = epsilon*epsilon_decay_rate     # update epsilon\n",
    "            s+=1\n",
    "            \n",
    "            if s == set_num:\n",
    "                break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your result here (Feel free to modify)\n",
    "# the result_p_mc should be a (set_num, )-numpy array that records the average reward of a set of episodes \n",
    "\n",
    "# put your result here\n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 15}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "figure(figsize=(12,6))\n",
    "ax = subplot(1,1,1)\n",
    "ax.plot(range(0,set_num), result_p_mc, linewidth=2, color='g')\n",
    "\n",
    "plt.ylabel(\"Set average reward\");\n",
    "plt.xlabel(\"Set number (2000 episodes simulated in each set)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use greedy policy of the trained Q function to control the carpole for 100 episode, \n",
    "# and plot the accumulate reward\n",
    "## Suggested flow (Feel free to modify and add)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "observation = env.reset()\n",
    "observation = observation([[0,2]])          # partially observed states\n",
    "# img = plt.imshow(env.render(mode='rgb_array')) \n",
    "\n",
    "count = 0\n",
    "while 1:\n",
    "    #     img.set_data(env.render(mode='rgb_array')) \n",
    "    #     display.display(plt.gcf())\n",
    "    #     display.clear_output(wait=True)\n",
    "    \n",
    "    current_state =                             # discretize the observation space\n",
    "    \n",
    "    action =                                    # choose action by greedy policy of the trained Q\n",
    "    \n",
    "    observation, reward, done, info = env.step(action)\n",
    "    observation = observation([[0,2]])          # partially observed states\n",
    "    \n",
    "    if done:\n",
    "        \n",
    "        observation = env.reset()\n",
    "        count +=1\n",
    "        \n",
    "        total_reward_p_mc =                                # record the accumulate reward until this episode\n",
    "        \n",
    "        if count==100:\n",
    "            break\n",
    "        \n",
    "font = {'weight' : 'bold',\n",
    "        'size'   : 15}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "figure(figsize=(12,6))\n",
    "ax = subplot(1,1,1)\n",
    "ax.plot(range(100), total_reward_p_mc,range(100), total_reward_mc) # compare with section 3.2\n",
    "\n",
    "plt.ylabel(\"Accumulate Reward\");\n",
    "plt.xlabel(\"Episode\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
